{
  "source_file": "backpropagation.md",
  "category": "backpropagation, Backpropagation explicado visualmente, Cómo funciona el algoritmo",
  "title": "Ejemplo simple de backpropagation",
  "generated_at": "2026-02-20 13:34:15",
  "model": "llama3:latest",
  "ok": true,
  "reason": "OK",
  "hierarchy": {
    "h1": "Backpropagation explicado visualmente",
    "h2": "Cómo funciona el algoritmo",
    "h3_raw": "Ejemplo simple de backpropagation"
  },
  "ollama_url": "http://127.0.0.1:11434/api/chat",
  "content": "# Ejemplo simple de backpropagation\n**Meta:** Entendiendo el algoritmo de backpropagation a través de un ejemplo simple.\n\nLa backpropagation es un algoritmo fundamental en la programación por aprendizaje automático, utilizado para entrenar redes neuronales. Aunque puede parecer complicado inicialmente, entender cómo funciona es crucial para implementarlo correctamente. En este artículo, vamos a explorar un ejemplo simple de backpropagation para una red neuronal con una sola neurona.\n\n**Cómo funciona el algoritmo**\n\nLa backpropagation se basa en calcular el gradiente del error entre la salida predicha y la salida deseada, para ajustar los pesos de las conexiones entre las neuronas. En un ejemplo simple, supongamos que queremos entrenar una red neuronal con una sola neurona que clasifique entradas como 0 o 1.\n\n**Ejemplo simple**\n\nSupongamos que nuestra red neuronal tiene una sola neurona con un peso inicial de 0.5 y un threshold (umbral) de 0.3. La entrada es un vector de dos elementos, [x1, x2]. La salida de la neurona se calcula como:\n\n`y = sigmoid(0.5*x1 + 0.5*x2 - 0.3)`\n\nDonde `sigmoid` es una función de activación que toma valores entre 0 y 1.\n\nPara entrenar esta red, necesitamos calcular el error entre la salida predicha y la salida deseada. Supongamos que la entrada es [0, 0] y la salida deseada es 0. La salida predicha sería:\n\n`y = sigmoid(0.5*0 + 0.5*0 - 0.3) ≈ 0.21`\n\n Como la salida deseada es 0, el error se calcula como:\n\n`error = (0 - 0.21) ≈ 0.79`\n\nAhora necesitamos calcular el gradiente del error con respecto a los pesos y el umbral. El gradiente de los pesos se calcula como:\n\n`dW/dx1 = d(0.5*x1 + 0.5*x2 - 0.3)/dx1 ≈ 0.5`\n\n`dW/dx2 = d(0.5*x1 + 0.5*x2 - 0.3)/dx2 ≈ 0.5`\n\nY el gradiente del umbral como:\n\n`du/db = d(sigmoid(0.5*x1 + 0.5*x2 - 0.3))/db ≈ -0.79*sigmoid'(0.5*x1 + 0.5*x2 - 0.3)`\n\nDonde `sigmoid'` es la derivada de la función de activación.\n\n**Errores típicos / trampas**\n\n1. **No entender el algoritmo**: La backpropagation puede parecer complicado, pero es fundamental comprender cómo funciona para implementarla correctamente.\n2. **No normalizar las entradas**: Es importante normalizar las entradas para evitar que las características más grandes dominen la red neuronal.\n3. **No utilizar un learning rate adecuado**: Un learning rate demasiado alto puede causar a la red neuronal perder el equilibrio y no aprender correctamente.\n\n**Checklist accionable**\n\n1. Entender cómo funciona la backpropagation en un ejemplo simple\n2. Normalizar las entradas para evitar distorsiones\n3. Utilizar un learning rate adecuado para evitar pérdida de equilibrio\n4. Implementar una función de activación correcta para la neurona\n5. Repetir el proceso de entrenamiento varias veces para ajustar los pesos y el umbral\n\n**Siguientes pasos**\n\n* Aprender a implementar la backpropagation en redes neuronales más complejas\n* Entender cómo utilizar optimizadores como SGD o Adam para ajustar los pesos y el umbral\n* Aprender a regularizar la red neuronal para evitar sobreajuste y pérdida de generalización"
}