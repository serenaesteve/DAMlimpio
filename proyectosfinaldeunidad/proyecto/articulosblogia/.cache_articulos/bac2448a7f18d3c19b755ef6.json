{
  "source_file": "backpropagation.md",
  "category": "backpropagation, Backpropagation explicado visualmente, El problema que resuelve backpropagation",
  "title": "Cómo se calculan los gradientes",
  "generated_at": "2026-02-20 13:15:40",
  "model": "llama3:latest",
  "ok": false,
  "reason": "Muy corto (~640 palabras) | Repair falló: No empieza con H1 (# ...)",
  "hierarchy": {
    "h1": "Backpropagation explicado visualmente",
    "h2": "El problema que resuelve backpropagation",
    "h3_raw": "Cómo se calculan los gradientes"
  },
  "ollama_url": "http://127.0.0.1:11434/api/chat",
  "content": "# Cómo se calculan los gradientes\n**Meta:** Entender cómo backpropagation calcula los gradientes para reducir el error en modelos de aprendizaje automático.\n\nLa backpropagation es un algoritmo fundamental en el aprendizaje automático, y calcular los gradientes es una parte crucial del proceso. En esta sección, exploraremos cómo se calculan los gradientes para entender mejor cómo funciona backpropagation.\n\n**Importancia de calcular gradientes**\n\nAntes de explicar cómo se calculan los gradientes, es importante comprender por qué son importantes. Los gradientes nos permiten saber en qué dirección y cuánto cambiar cada peso (parametro del modelo) para reducir el error entre la salida predicha y la salida real. En otras palabras, los gradientes nos indican cómo ajustar el modelo para mejorar su rendimiento.\n\n**Regla de la cadena**\n\nLa regla de la cadena es una herramienta fundamental para calcular los gradientes en backpropagation. La idea básica es aplicar la regla de la cadena iterativamente a cada capa del modelo, desde la salida hasta la entrada.\n\nSupongamos que tenemos un modelo con dos capas: una capa oculta y una capa de salida. La regla de la cadena se aplica de la siguiente manera:\n\n1. Calculamos el error en la salida (`E = (y_true - y_pred)^2`, donde `y_true` es la salida real y `y_pred` es la salida predicha).\n2. Aplicamos la regla de la cadena a la capa de salida para calcular el gradiente del error con respecto al output (`dL/dy`) de la capa.\n3. Luego, aplicamos la regla de la cadena a la capa oculta para calcular el gradiente del error con respecto al output (`dL/dh`) de la capa.\n4. Finalmente, aplicamos la regla de la cadena a la entrada para calcular el gradiente del error con respecto a la entrada (`dL/dx`).\n\n**Ejemplo**\n\nSupongamos que tenemos un modelo con una capa oculta con 2 neuronas y una capa de salida con 1 neurona. La función de activación en la capa oculta es la tangente hiperbólica (tanh) y la función de activación en la capa de salida es la sigmoidal.\n\n```python\nimport numpy as np\n\n# Weights and biases for the hidden layer\nW1 = np.random.rand(2, 784)\nb1 = np.zeros((2,))\n\n# Weights and biases for the output layer\nW2 = np.random.rand(10, 2)\nb2 = np.zeros((10,))\n\n# Input data\nx = np.array([[0.5, 0.5]])\n\n# Forward pass\nh = np.tanh(np.dot(x, W1) + b1)\ny_pred = np.sigmoid(np.dot(h, W2) + b2)\n\n# Error calculation\nE = (np.array([1])) - y_pred\n\n# Backward pass\ndL_dy = 2 * E\ndL_dh = dL_dy * (1 - h**2) * W2.T\ndL_dx = np.dot(dL_dh, W1.T)\n\nprint(\"Gradient of the error with respect to the input:\", dL_dx)\n```\n\nEn este ejemplo, estamos calculando el gradiente del error con respecto a la entrada (`dL/dx`) utilizando la regla de la cadena. La salida predicha (`y_pred`) se utiliza para calcular el error (`E`), y luego se aplica la regla de la cadena iterativamente a cada capa del modelo.\n\n**Errores típicos / trampas**\n\n1. **Error en la implementación de la regla de la cadena**: Asegúrese de que esté aplicando correctamente la regla de la cadena en su código.\n2. **Reducción excesiva de los gradientes**: Asegúrese de que no esté reduciendo demasiado rápido los gradientes, lo que puede llevar a un ajuste incorrecto del modelo.\n3. **Gradientes muy pequeños**: Verifique si los gradientes son muy pequeños y ajuste el learning rate accordingly.\n\n**Checklist accionable**\n\n1. Asegúrese de entender cómo se calculan los gradientes en backpropagation.\n2. Practique implementando la regla de la cadena en su código.\n3. Verifique que no esté reduciendo demasiado rápido los gradientes.\n4. Ajuste el learning rate según sea necesario.\n\n**Siguientes pasos**\n\n* Aprender a implementar backpropagation en diferentes frameworks y lenguajes de programación.\n* Practicar con ejercicios y problemas para consolidar su comprensión de la regla de la cadena.\n* Explorar cómo se pueden utilizar los gradientes para ajustar el modelo y mejorar su rendimiento."
}