{
  "source_file": "redes_neuronales.md",
  "category": "redes_neuronales, Redes neuronales artificiales, Conceptos básicos",
  "title": "Funciones de activación",
  "generated_at": "2026-02-20 13:27:23",
  "model": "llama3:latest",
  "ok": false,
  "reason": "Muy corto (~467 palabras) | Repair falló: Muy corto (~703 palabras)",
  "hierarchy": {
    "h1": "Redes neuronales artificiales",
    "h2": "Conceptos básicos",
    "h3_raw": "Funciones de activación"
  },
  "ollama_url": "http://127.0.0.1:11434/api/chat",
  "content": "# Funciones de activación\n**Meta:** Descubre cómo las funciones de activación aportan no linealidad a tus redes neuronales y aprenden patrones complejos.\n\nLas funciones de activación son un elemento fundamental en las redes neuronales artificiales. Introducen no linealidad en la red, lo que permite aprender patrones complejos y mejorar el rendimiento del modelo. En esta sección, exploraremos los conceptos básicos de las funciones de activación y cómo utilizarlas para crear modelos efectivos.\n\nLas funciones de activación toman como entrada el resultado de una capa neuronal y devuelven un valor entre 0 y 1 o -1 y 1, dependiendo del tipo de función. Esto permite a la red neuronal aprender patrones complejos y no lineales en los datos.\n\nAlgunos ejemplos comunes de funciones de activación son:\n\n* ReLU (Rectified Linear Unit): `f(x) = max(0, x)`\n* Sigmoid: `f(x) = 1 / (1 + exp(-x))`\n* Tanh: `f(x) = 2 / (1 + exp(-2x)) - 1`\n\nCada una de estas funciones tiene sus ventajas y desventajas. Por ejemplo, ReLU es rápida y eficiente, pero puede causar problemas de vanishing gradients en las capas superiores. Sigmoid se utiliza comúnmente como función de salida para problemas binarios, pero puede ser lenta y no lineal.\n\nUn ejemplo simple de cómo utilizar una función de activación en un modelo neuronal es el siguiente:\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Crear una capa neuronal con una función de activación sigmoid\nweights = np.array([[0.5], [0.2]])\nbias = np.array([0.3])\nactivation = sigmoid(np.dot(inputs, weights) + bias)\n```\nEn este ejemplo, se define una función de activación sigmoid y se utiliza para aplicar la función de activación a los resultados de una capa neuronal.\n\nErrores típicos y trampas:\n\n1. Utilizar la misma función de activación en todas las capas: es importante elegir la función de activación adecuada para cada capa, dependiendo del problema y de los datos.\n2. No utilizar suficientes capas y funciones de activación: una red neuronal con demasiadas capas puede ser lenta y no lineal.\n3. Utilizar la función de activación sigmoid en las capas superiores: aunque sigmoid es útil como función de salida, puede ser lenta y no lineal para utilizarla en las capas superiores.\n\nChecklist accionable:\n\n1. Elegir la función de activación adecuada para cada capa.\n2. Utilizar suficientes capas y funciones de activación.\n3. No utilizar la función de activación sigmoid en las capas superiores.\n4. Experimentar con diferentes funciones de activación y ver cuál funciona mejor para el problema.\n5. Aprender a analizar y entender los resultados del modelo.\n\nSiguientes pasos:\n\n* Explorar diferentes algoritmos de optimización para ajustar los parámetros de la red neuronal.\n* Aprender a utilizar técnicas de regularización para evitar sobreajuste y mejorar el rendimiento del modelo.\n* Experimentar con diferentes arquitecturas de redes neuronales y ver cuál funciona mejor para el problema."
}